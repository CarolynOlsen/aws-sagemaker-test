{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "# Remove columns that have a STD less than\n",
    "MINIMUM_STD = 0.00001\n",
    "\n",
    "# Sensor baseline thresholds to loop through\n",
    "SENSOR_BASELINE_THRESHOLDS = [50]\n",
    "\n",
    "# Define columns\n",
    "DF_COLUMNS = [\"ENGINE_NUMBER\", \"TIME_IN_CYCLES\"] + \\\n",
    "             [\"OPERATIONAL_SETTING_{}\".format(x) for x in range(1,4)] + \\\n",
    "             [\"SENSOR_MEASUREMENT_{}\".format(x) for x in range(1,24)]\n",
    "\n",
    "# Define data paths and data names\n",
    "DATA_PATH = \"/home/ec2-user/SageMaker/aws-sagemaker-test/data/\"\n",
    "OUTDATA_PATH = \"/home/ec2-user/SageMaker/aws-sagemaker-test/engine_data/\"\n",
    "DS_FILENAME = DATA_PATH + \"{}_FD00{}.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining funcitons to load data and run feature engineering stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carolyns sensor baseline.. \n",
    "def get_sensor_baseline(df):\n",
    "    for thres in SENSOR_BASELINE_THRESHOLDS:\n",
    "        for column in [col for col in df.columns if 'SENSOR_MEASUREMENT' in col]:\n",
    "            data = df[['TIME_IN_CYCLES', column]]\n",
    "\n",
    "            # Get baseline average value (if <50 cycles, make it average to that point in time, \n",
    "            # Else make it average of first 50 cycles)\n",
    "            data.loc[:,'SENSOR_BASELINE_AVG'] = np.where(\n",
    "                    data['TIME_IN_CYCLES'] < thres,\n",
    "                    data[column].expanding(min_periods=0, axis=0).mean(),\n",
    "                    np.mean(df[column][0:thres])\n",
    "            )\n",
    "\n",
    "            # Get baseline standard deviation, with same logic\n",
    "            data.loc[:,'SENSOR_BASELINE_STD'] = np.where(\n",
    "                    data['TIME_IN_CYCLES'] < thres,\n",
    "                    data[column].expanding(min_periods=0, axis=0).std(),\n",
    "                    np.std(df[column][0:thres])\n",
    "            )\n",
    "\n",
    "           # Define new column\n",
    "            df.loc[:,'BASELINE_{}TS_{}'.format(thres, column)] = (\n",
    "                    (data[column] - data.SENSOR_BASELINE_AVG) / data.SENSOR_BASELINE_STD.fillna(1)\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # Get a rolling avg of the sensor values, too\n",
    "            df.loc[:,'ROLLINGMEAN_{}TS_{}'.format(thres, column)] = (\n",
    "                data[column].rolling(10).mean()\n",
    "            )\n",
    "            \n",
    "            # And a rolling avg of std dev from baseline\n",
    "            df.loc[:,'BASE_ROLL_{}TS_{}'.format(thres, column)] = (\n",
    "                df.loc[:,'BASELINE_{}TS_{}'.format(thres, column)].rolling(10).mean()\n",
    "            )\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Function to load some RUL data\n",
    "def load_rul(data_path):  \n",
    "    df = pd.read_csv(data_path, header=None, names=['RUL'])\n",
    "    df['ENGINE_NUMBER'] = np.arange(1, len(df) + 1)\n",
    "    return df\n",
    "\n",
    "# Function to filter data that doesn't meet a certain criteria\n",
    "def drop_bad_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Remove columns where the STD is less than MINIMUM_STD (only sensor data... not settings)\n",
    "    \"\"\"\n",
    "    df = dataframe.describe().T.reset_index()\n",
    "    for _,data in df.iterrows():\n",
    "        if abs(data['std']) <= MINIMUM_STD and 'SENSOR' in data['index']:\n",
    "            del dataframe[data['index']]\n",
    "    return dataframe.reset_index(drop=True)\n",
    "\n",
    "# Function to load some data\n",
    "def load_data(data_path, filter_data=False, feature_engineer=False):  \n",
    "    \"\"\"\n",
    "    Load data in\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path, sep=' ', header=None, names=DF_COLUMNS)\n",
    "    data = data.drop(DF_COLUMNS[-2:], axis=1)\n",
    "#     data['TIME'] = pd.date_range('1/1/2000', periods=data.shape[0], freq='600s')\n",
    "    if filter_data:\n",
    "        data = drop_bad_columns(data)\n",
    "    if feature_engineer:\n",
    "        data = get_sensor_baseline(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, test, RUL data for dataset 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset 1\n",
    "train001 = load_data(DS_FILENAME.format('train', '1'), filter_data=True, feature_engineer=True)\n",
    "test001 = load_data(DS_FILENAME.format('test', '1'), filter_data=True, feature_engineer=True)\n",
    "rul001 = load_rul(DS_FILENAME.format('RUL', '1'))\n",
    "\n",
    "# Load dataset 2\n",
    "train002 = load_data(DS_FILENAME.format('train', '2'), filter_data=True, feature_engineer=True)\n",
    "test002 = load_data(DS_FILENAME.format('test', '2'), filter_data=True, feature_engineer=True)\n",
    "rul002 = load_rul(DS_FILENAME.format('RUL', '2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering on training\n",
    "Define function to find the max number of cycles per engine, then for each cycle determine how many RUL remain.\n",
    "\n",
    "We delete ENGINE_NUMBER becuase we do not want it as an input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rul(df):\n",
    "    # Get max cycle per engine\n",
    "    max_cycle = df.groupby('ENGINE_NUMBER').TIME_IN_CYCLES.max().reset_index()\n",
    "    max_cycle.columns = ['ENGINE_NUMBER', 'MAX_CYCLES']\n",
    "        \n",
    "    # Merge onto data and reorganize the columns\n",
    "    df = pd.merge(df, max_cycle, on='ENGINE_NUMBER')\n",
    "    df['RUL'] = df.MAX_CYCLES - df.TIME_IN_CYCLES\n",
    "    del df['MAX_CYCLES']\n",
    "    df = df[['RUL'] + [col for col in df.columns if col != 'RUL']]\n",
    "    del df['ENGINE_NUMBER']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RUL per engine for each cycle\n",
    "train001 = compute_rul(train001)\n",
    "train002 = compute_rul(train002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering on testing\n",
    "Define function to combine the test file with the RUL data, and select the last cycle per engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_test_rul(test, rul):\n",
    "    # Only keep last row per engine number\n",
    "    df = test[test.groupby('ENGINE_NUMBER')['TIME_IN_CYCLES'].transform(max) == test['TIME_IN_CYCLES']]\n",
    "    df = pd.merge(df, rul, on='ENGINE_NUMBER')\n",
    "    df = df[['RUL'] + [col for col in df.columns if col != 'RUL']]\n",
    "    del df['ENGINE_NUMBER']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run combine code\n",
    "test001 = combine_test_rul(test001, rul001)\n",
    "test002 = combine_test_rul(test002, rul002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out new data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'bryan-predictive-maintenance'\n",
    "\n",
    "def write_to_csv(df, fname):\n",
    "    # Change column order and save file locally\n",
    "    df.to_csv(fname, index=False, header=False)\n",
    "    \n",
    "    # Create connection\n",
    "    s3conn = boto3.client('s3')\n",
    "    \n",
    "    # Write file\n",
    "    outfile = 'sagemaker/{}'.format(fname.split('/')[-1])\n",
    "    s3conn.put_object(\n",
    "            Body=open(fname),\n",
    "            Bucket=bucket,\n",
    "            Key=outfile\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all files to csv\n",
    "write_to_csv(train001, OUTDATA_PATH + 'train001.csv')\n",
    "write_to_csv(train002, OUTDATA_PATH + 'train002.csv')\n",
    "write_to_csv(test001, OUTDATA_PATH + 'test001.csv')   \n",
    "write_to_csv(test002, OUTDATA_PATH + 'test002.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
